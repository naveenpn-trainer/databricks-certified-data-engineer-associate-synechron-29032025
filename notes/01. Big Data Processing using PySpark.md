# Big Data Processing using Apache Spark

## Introduction

> Apache Spark is an **In-memory** cluster computing framework designed to handle a <mark>wide range of big data workloads.</mark>

1. High performance Batch computation
2. Data Integration and ETL
3. Machine learning analytics
4. Real-time stream processing
5. Graph computation

* Apache Spark is natively written using Scala

## PySpark

> PySpark is the Python API for Apache Spark

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXf6z0Qq3ABnu18HgfOgumpbAmAztvxY0QJvnoikDGyahN3S2DwZKtp1mDjz4f_T5pt7h_4JGgDxsKDcunjngRhVhvP1rxBaCU5r59VgyvBjzVHEFvVGioRoM9Kk_iNAvD_w1mmPk-TXGe0HmXW2ig6GQa7g?key=_he-T4Jq934AhrSZa-Be-g)



## Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeEouEMUvxYbpvvYhYamcqmzzx0UDlTCAxjwjkJdiCiFGRAtl-8ynYg7uDedWQorvM64_3vCWu7r9diq_SsrbGxt-p7H1H1E-f0-sRruT6e9rBJ2tgNfPgUYZ9QPX069wmGKWxH0I8-96h28WasdwBSsedz?key=_he-T4Jq934AhrSZa-Be-g)

## Spark Interactive Shell

1. Spark Shell

   - Language is Scala

   - ```
     $> spark-shell
     ```

     

2. PySpark Shell

   - Language is python

   - ```
     $> pyspark
     ```

* Spark interactive shell is only for learning purpose

## Spark APIs

1. Low Level API (RDD's)
2. High Level API (Spark SQL)

## Building a Spark Application

1. Load the data into spark data structure
2. Process the data
3. Store the data



```
String data = "Naveen"
```



![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcmFxz2TYH2sDakrm6Thq-Drfxq3IGtsbINFVn0iUQdtNbWOhVyPN4fuw744mOiq1FWJR4TvOOugHiyvRoHwZB7QprJ1xbSsUSJJfeqrX0QFqyWptz4d43FFuoMrwPpaIvztkgYZQDyaPqOcFx74DJkxRE?key=_he-T4Jq934AhrSZa-Be-g)



## RDD's (Low Level API's )

> Resilient Distributed Dataset is a fundamental data structure of Apache spark.

**Partitions**

> A Partition in spark is a logical division of data stored on a node in a cluster

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcU00tASuwrltRE7N_rdsJbNh1tFqmpBn1it7rNOY5BHw8qnHv27bdLlNM7OtmbW9yehm1ZRwYuIHAU_9lwA6KYijFnEf6NKJuT8Z0gSW59mu2ojrwMB9OJIV6HJ0YCXCflT2C8hAtLPLNPYT5R1DZIIUkV?key=Dxp7lTxgvspH2ig-I7LuEw)



### RDD Creation

There are two popular ways to create an RDD

1. Create an RDD from Collection

   ```
   L = list(range(1,101))
   numbers_rdd = sc.parallelize(L)
   type(numbers_rdd)
   ```

   

2. Create an RDD from External source

* All the methods to create an RDD is present inside SparkContext (sc)

  ```
  users_rdd = sc.textFile("c:/users.csv")
  
  user_df = spark.read.csv("c:/users.csv")
  ```

### RDD/DataFrame Operations

Two types of operations

1. Transformation
2. Actions

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeLB7ZPQSTocLo4dVGfDO1D0v4e-_eH7BAkJ5MDhORsqCdxUS5YBVZEAdii8oVONpVZCS5Pt_60HCI9tJvphtVvDICDOvlGdFcPWCjnJmN78Wo_ryYKjC6c9vYfUhS_kURwuDYwC__EP0LNxPe8OixzmDaw?key=Dxp7lTxgvspH2ig-I7LuEw)



## High Level API's (Spark SQL)

> Spark SQL is one of the module in the spark ecosystem to query and analyze structured and semi-structured 

> A DataFrame is a distributed collection of data organized into named columns

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdDmqeyrE5x0D0Lh7UvnYyDsRn_c8bcueFda4cxRHL8bzturBNJfPGHTinJuNbcByy5AdC0WMb9aaLMXNb_wZiY6IEVYihQp4Tbc89xdaCRvsADW9bgEwbuJe7tHdOG0ylK0-uwMve7eHQXdEm4yjv2NQeG?key=yGW25KMloT80Lch6YWjT9A)

